# ============================================================
#  OpenGrant — LLM Provider Configuration
#  Choose ANY provider below. Set only what you have an API key for.
# ============================================================

# --- PRIMARY PROVIDER (uncomment one) ---
# Option 1: GROQ (FREE, Recommended) — https://console.groq.com
GROQ_API_KEY=your_groq_api_key_here

# Option 2: OpenAI (ChatGPT) — https://platform.openai.com/api-keys
# OPENAI_API_KEY=sk-...

# Option 3: Anthropic (Claude) — https://console.anthropic.com
# ANTHROPIC_API_KEY=sk-ant-...

# Option 4: Google Gemini — https://aistudio.google.com/apikey
# GEMINI_API_KEY=your_gemini_api_key_here

# Option 5: NVIDIA NIM (FREE) — https://build.nvidia.com
# NVIDIA_API_KEY=nvapi-...

# Option 6: OpenRouter (Aggregated Models) — https://openrouter.ai
# OPENROUTER_API_KEY=sk-or-...

# Option 7: Local Ollama (Free, Private) — https://ollama.ai
# OLLAMA_BASE_URL=http://localhost:11434/v1
# OLLAMA_MODEL=llama2

# --- GitHub Configuration (Optional) ---
GITHUB_TOKEN=your_github_personal_access_token_here

# --- Database & Server ---
DATABASE_URL=sqlite:///./fund_matcher.db
BACKEND_PORT=8765
ENVIRONMENT=development
FRONTEND_URL=http://localhost:5173
RATE_LIMIT_PER_MINUTE=20

# --- PRODUCTION DEPLOYMENT ---
# For public beta, set ENVIRONMENT=production to:
#   - Disable /docs and /redoc endpoints
#   - Enforce stricter security headers
#   - Hide debug information
# Example for production:
# ENVIRONMENT=production
# FRONTEND_URL=https://yourdomain.com
# RATE_LIMIT_PER_MINUTE=10

# --- Usage Instructions ---
# 1. Copy this file: cp .env.example .env
# 2. Uncomment the provider you want to use (or set as env var)
# 3. Paste your API key
# 4. Save and restart the backend
# 5. The system will auto-detect and use your chosen provider
#
# If multiple providers are configured, it will use the first available one
# in this priority: GROQ > OPENAI > ANTHROPIC > GEMINI > NVIDIA > OPENROUTER > OLLAMA
